{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic example: Use \"Weights and biases\" with JAX\n",
    "\n",
    "In this notebook, we track the training process of a neural ODE regression (programmed using JAX, Diffrax, Equinox, and Optax) using [weight and biases](https://wandb.ai/site).\n",
    "\n",
    "âš  The NeuralODE code below is copied and then altered from the example provided in the [Diffrax  git repo](https://docs.kidger.site/diffrax/examples/neural_ode/). If you happen to use Diffrax in your academic research kindly [cite the library](https://docs.kidger.site/diffrax/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "jaxlib is version 0.3.22, but this version of jax requires version >= 0.4.4.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdiffrax\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mequinox\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01meqx\u001B[39;00m  \u001B[38;5;66;03m# https://github.com/patrick-kidger/equinox\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/diffrax/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madjoint\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      2\u001B[0m     AbstractAdjoint,\n\u001B[1;32m      3\u001B[0m     BacksolveAdjoint,\n\u001B[1;32m      4\u001B[0m     DirectAdjoint,\n\u001B[1;32m      5\u001B[0m     ImplicitAdjoint,\n\u001B[1;32m      6\u001B[0m     RecursiveCheckpointAdjoint,\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautocitation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m citation, citation_rules\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbrownian\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AbstractBrownianPath, UnsafeBrownianPath, VirtualBrownianTree\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/diffrax/adjoint.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, Optional\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mequinox\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01meqx\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mequinox\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minternal\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01meqxi\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/equinox/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental, internal, nn\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mad\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      3\u001B[0m     filter_closure_convert,\n\u001B[1;32m      4\u001B[0m     filter_custom_jvp,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     filter_vjp,\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m filter_pure_callback\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/equinox/experimental/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatch_norm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchNorm\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspectral_norm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SpectralNorm\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstateful\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_state, set_state, StateIndex\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/equinox/experimental/batch_norm.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Hashable, Optional, Sequence, Union\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlax\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlax\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/jax/__init__.py:35\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _cloud_tpu_init\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config \u001B[38;5;28;01mas\u001B[39;00m _config_module\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _config_module\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/jax/config.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2018 The JAX Authors.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# TODO(phawkins): fix users of this alias and delete this file.\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/jax/_src/config.py:25\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, List, Callable, Hashable, NamedTuple, Iterator, Optional\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jax_jit\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transfer_guard_lib\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/jax/_src/lib/__init__.py:74\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     71\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _jaxlib_version\n\u001B[1;32m     73\u001B[0m version_str \u001B[38;5;241m=\u001B[39m jaxlib\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;241m.\u001B[39m__version__\n\u001B[0;32m---> 74\u001B[0m version \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_jaxlib_version\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m  \u001B[49m\u001B[43mjax_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__version__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m  \u001B[49m\u001B[43mjaxlib_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjaxlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__version__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m  \u001B[49m\u001B[43mminimum_jaxlib_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_minimum_jaxlib_version\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# uses instructions that are present on this machine.\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/jax-cpu/lib/python3.8/site-packages/jax/_src/lib/__init__.py:63\u001B[0m, in \u001B[0;36mcheck_jaxlib_version\u001B[0;34m(jax_version, jaxlib_version, minimum_jaxlib_version)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _jaxlib_version \u001B[38;5;241m<\u001B[39m _minimum_jaxlib_version:\n\u001B[1;32m     61\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjaxlib is version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjaxlib_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but this version \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     62\u001B[0m          \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mof jax requires version >= \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mminimum_jaxlib_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 63\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _jaxlib_version \u001B[38;5;241m>\u001B[39m _jax_version:\n\u001B[1;32m     66\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjaxlib version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjaxlib_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is newer than and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     67\u001B[0m          \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mincompatible with jax version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjax_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     68\u001B[0m          \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupdate your jax and/or jaxlib packages.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: jaxlib is version 0.3.22, but this version of jax requires version >= 0.4.4."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use...\n",
    "- JAX, providing linear algebra with automatic differentiation and GPU acceleration\n",
    "- Equinox to build neural networks\n",
    "- Optax for optimisers\n",
    "- Diffrax for ODE solvers\n",
    "\n",
    "Define a NN that models an ODE..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrap the ODE solve into a model..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralODE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return solution.ys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Toy dataset of nonlinear oscillators. Sample paths look like deformed sines and cosines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jrandom.uniform(key, (2,), minval=-0.6, maxval=1)\n",
    "\n",
    "    def f(t, y, args):\n",
    "        x = y / (1 + y)\n",
    "        return jnp.stack([x[1], -x[0]], axis=-1)\n",
    "\n",
    "    solver = diffrax.Tsit5()\n",
    "    dt0 = 0.1\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(f), solver, ts[0], ts[-1], dt0, y0, saveat=saveat\n",
    "    )\n",
    "    ys = sol.ys\n",
    "    return ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 10, 100)\n",
    "    key = jrandom.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Main entry point. Try running main()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(\n",
    "        dataset_size=256,\n",
    "        batch_size=32,\n",
    "        lr_strategy=(3e-3, 3e-3),\n",
    "        steps_strategy=(500, 500),\n",
    "        length_strategy=(0.1, 1),\n",
    "        width_size=64,\n",
    "        depth=2,\n",
    "        seed=5678,\n",
    "        plot=True,\n",
    "        print_every=100,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key = jrandom.split(key, 3)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    _, length_size, data_size = ys.shape\n",
    "\n",
    "    model = NeuralODE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "    # Standard training loop.\n",
    "    #\n",
    "    # Until step 500 we train on only the first 10% of each time series.\n",
    "    # This is a standard trick to avoid getting caught in a local minimum.\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def grad_loss(model, ti, yi):\n",
    "        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "        return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(ti, yi, model, opt_state):\n",
    "        loss, grads = grad_loss(model, ti, yi)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
    "        optim = optax.adabelief(lr)\n",
    "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        _ts = ts[: int(length_size * length)]\n",
    "        _ys = ys[:, : int(length_size * length)]\n",
    "        for step, (yi,) in zip(\n",
    "                range(steps), dataloader((_ys,), batch_size, key=loader_key)\n",
    "        ):\n",
    "            start = time.time()\n",
    "            loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "            end = time.time()\n",
    "            if (step % print_every) == 0 or step == steps - 1:\n",
    "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(ts, ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "        plt.plot(ts, ys[0, :, 1], c=\"dodgerblue\")\n",
    "        model_y = model(ts, ys[0, 0])\n",
    "        plt.plot(ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "        plt.plot(ts, model_y[:, 1], c=\"crimson\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"neural_ode.png\")\n",
    "        plt.show()\n",
    "\n",
    "    return ts, ys, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts, ys, model = main()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
